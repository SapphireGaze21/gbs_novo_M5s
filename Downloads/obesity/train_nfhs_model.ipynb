{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c358d9ba-f214-4c92-b55f-4a12d008ad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (107, 28)\n",
      "Fitting Linear Regression (no hyperparams)...\n",
      "LR CV RMSE: 2.9044041598203365\n",
      "\n",
      "Running RandomForest search (fast)...\n",
      "RF best (cv) RMSE: 4.105862046811917\n",
      "RF best params: {'model__n_estimators': 100, 'model__min_samples_split': 4, 'model__max_depth': 5}\n",
      "\n",
      "Running GradientBoosting search (fast)...\n",
      "GBR best (cv) RMSE: 3.9993716689582395\n",
      "GBR best params: {'model__n_estimators': 100, 'model__max_depth': 3, 'model__learning_rate': 0.2}\n",
      "\n",
      "CV RMSE summary: {'LinearRegression': 2.9044041598203365, 'RandomForest': 4.105862046811917, 'GradientBoosting': 3.9993716689582395}\n",
      "Selected: LinearRegression\n",
      "\n",
      "Test metrics: RMSE=3.4133, MAE=2.7454, R2=0.8973\n",
      "Saved model to /Users/sahitipotini/Downloads/obesity/best_nfhs_model.pkl\n",
      "Saved test predictions to /Users/sahitipotini/Downloads/obesity/predictions_test_set.csv\n",
      "Saved all rows predictions to /Users/sahitipotini/Downloads/obesity/all_predictions.csv\n",
      "\n",
      "Residuals: mean: -0.4979669189639321 std: 3.456218377231901\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Predictive modeling script (fast / notebook-friendly)\n",
    "# Save as `train_nfhs_model.py` or run in a notebook cell.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------- CONFIG ----------\n",
    "INPUT_CSV = \"/Users/sahitipotini/Downloads/obesity/NFHS_5_cleaned_data.csv\"   # change path if needed\n",
    "TARGET = \"women_overweight_obese_pct\"                     # target variable\n",
    "STATE_COL = \"state\"    # high-cardinality -> we drop for modeling to avoid overfitting\n",
    "# ---------------------------\n",
    "\n",
    "# Load data (attempt utf-8 then latin-1)\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
    "except Exception:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding='latin-1')\n",
    "\n",
    "print(\"Loaded:\", df.shape)\n",
    "\n",
    "# Verify target is present\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Target {TARGET} not found. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Feature selection: use all except state and target\n",
    "features = [c for c in df.columns if c not in (TARGET, STATE_COL)]\n",
    "# drop any constant columns\n",
    "features = [c for c in features if df[c].nunique() > 1]\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# Quick train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing: numeric scaling + one-hot for categorical (area)\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Simple pipelines\n",
    "lr_pipe = Pipeline(steps=[('pre', preprocessor), ('model', LinearRegression())])\n",
    "rf_pipe = Pipeline(steps=[('pre', preprocessor), ('model', RandomForestRegressor(random_state=42))])\n",
    "gbr_pipe = Pipeline(steps=[('pre', preprocessor), ('model', GradientBoostingRegressor(random_state=42))])\n",
    "\n",
    "# Lightweight randomized search grids (quick)\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": [50, 100, 150],\n",
    "    \"model__max_depth\": [None, 5, 10],\n",
    "    \"model__min_samples_split\": [2, 4, 6]\n",
    "}\n",
    "gbr_param_dist = {\n",
    "    \"model__n_estimators\": [50, 100],\n",
    "    \"model__learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"model__max_depth\": [3, 4, 5]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Fast RandomizedSearch (n_iter small to save time)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_search = RandomizedSearchCV(rf_pipe, rf_param_dist, n_iter=6, scoring='neg_mean_squared_error', cv=cv, random_state=42, n_jobs=-1)\n",
    "gbr_search = RandomizedSearchCV(gbr_pipe, gbr_param_dist, n_iter=6, scoring='neg_mean_squared_error', cv=cv, random_state=42, n_jobs=-1)\n",
    "\n",
    "print(\"Fitting Linear Regression (no hyperparams)...\")\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "lr_cv_rmse = np.sqrt(-cross_val_score(lr_pipe, X_train, y_train, cv=cv, scoring='neg_mean_squared_error').mean())\n",
    "print(\"LR CV RMSE:\", lr_cv_rmse)\n",
    "\n",
    "print(\"\\nRunning RandomForest search (fast)...\")\n",
    "rf_search.fit(X_train, y_train)\n",
    "rf_best_rmse = np.sqrt(-rf_search.best_score_)\n",
    "print(\"RF best (cv) RMSE:\", rf_best_rmse)\n",
    "print(\"RF best params:\", rf_search.best_params_)\n",
    "\n",
    "print(\"\\nRunning GradientBoosting search (fast)...\")\n",
    "gbr_search.fit(X_train, y_train)\n",
    "gbr_best_rmse = np.sqrt(-gbr_search.best_score_)\n",
    "print(\"GBR best (cv) RMSE:\", gbr_best_rmse)\n",
    "print(\"GBR best params:\", gbr_search.best_params_)\n",
    "\n",
    "# Compare and select best\n",
    "results = {\"LinearRegression\": lr_cv_rmse, \"RandomForest\": rf_best_rmse, \"GradientBoosting\": gbr_best_rmse}\n",
    "best_name = min(results, key=results.get)\n",
    "print(\"\\nCV RMSE summary:\", results)\n",
    "print(\"Selected:\", best_name)\n",
    "\n",
    "best_model = {\"LinearRegression\": lr_pipe, \"RandomForest\": rf_search.best_estimator_, \"GradientBoosting\": gbr_search.best_estimator_}[best_name]\n",
    "\n",
    "# Fit best model on full training data (ensures final fit)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))   # fixed\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest metrics: RMSE={rmse_test:.4f}, MAE={mae_test:.4f}, R2={r2_test:.4f}\")\n",
    "\n",
    "\n",
    "# Feature importances for tree models\n",
    "def get_feature_names(preprocessor):\n",
    "    num_feats = numeric_features\n",
    "    cat_feats = []\n",
    "    if categorical_features:\n",
    "        ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_feats = ohe.get_feature_names_out(categorical_features).tolist()\n",
    "    return num_feats + cat_feats\n",
    "\n",
    "if best_name in (\"RandomForest\", \"GradientBoosting\"):\n",
    "    model_obj = best_model.named_steps['model']\n",
    "    feat_names = get_feature_names(best_model.named_steps['pre'])\n",
    "    importances = model_obj.feature_importances_\n",
    "    fi = pd.DataFrame({'feature': feat_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop features:\\n\", fi.head(15))\n",
    "    fi.to_csv(\"/mnt/data/feature_importances.csv\", index=False)\n",
    "\n",
    "# Save model and predictions\n",
    "joblib.dump(best_model, \"/Users/sahitipotini/Downloads/obesity/best_nfhs_model.pkl\")\n",
    "print(\"Saved model to /Users/sahitipotini/Downloads/obesity/best_nfhs_model.pkl\")\n",
    "\n",
    "test_out = X_test.copy()\n",
    "test_out[TARGET] = y_test.values\n",
    "test_out[\"pred_\" + TARGET] = y_pred\n",
    "test_out.to_csv(\"/Users/sahitipotini/Downloads/obesity/predictions_test_set.csv\", index=False)\n",
    "print(\"Saved test predictions to /Users/sahitipotini/Downloads/obesity/predictions_test_set.csv\")\n",
    "\n",
    "# Full-data predictions\n",
    "df_full = df.copy()\n",
    "df_full[\"pred_\" + TARGET] = best_model.predict(df_full[features])\n",
    "df_full.to_csv(\"/Users/sahitipotini/Downloads/obesity/all_predictions.csv\", index=False)\n",
    "print(\"Saved all rows predictions to /Users/sahitipotini/Downloads/obesity/all_predictions.csv\")\n",
    "\n",
    "# Quick residual diagnostics (print)\n",
    "resid = y_test - y_pred\n",
    "print(\"\\nResiduals: mean:\", resid.mean(), \"std:\", resid.std())\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ebaa64-f40e-4787-bb67-cb4d624593d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (107, 28)\n",
      "LinearRegression CV RMSE: 2.9044\n",
      "Ridge best params: {'model__alpha': 1}\n",
      "Ridge CV RMSE: 2.8398\n",
      "Lasso best params: {'model__alpha': 0.01}\n",
      "Lasso CV RMSE: 2.7830\n",
      "RandomForest best params: {'model__n_estimators': 100, 'model__min_samples_split': 4, 'model__max_depth': 5}\n",
      "RandomForest CV RMSE: 4.1059\n",
      "GradientBoosting best params: {'model__n_estimators': 100, 'model__max_depth': 3, 'model__learning_rate': 0.05}\n",
      "GradientBoosting CV RMSE: 4.0959\n",
      "\n",
      "CV RMSE summary: {'LinearRegression': 2.9044041598203365, 'Ridge': 2.8397905312819396, 'Lasso': 2.78300734475657, 'RandomForest': 4.105862046811917, 'GradientBoosting': 4.0958943913618295}\n",
      "Selected best model: Lasso\n",
      "\n",
      "Test metrics for Lasso: RMSE=3.3608, MAE=2.7230, R2=0.9005\n",
      "Model saved as best_nfhs_model.pkl\n",
      "Predictions saved: predictions_test_set.csv & all_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Final Predictive Modeling Script for NFHS-5 dataset\n",
    "# Includes: LinearRegression, Ridge, Lasso, RandomForest, GradientBoosting\n",
    "# Selects best based on CV RMSE and evaluates on test set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------- CONFIG ----------\n",
    "INPUT_CSV = \"/Users/sahitipotini/Downloads/obesity/NFHS_5_cleaned_data.csv\"\n",
    "TARGET = \"women_overweight_obese_pct\"            # target variable\n",
    "STATE_COL = \"state\"                              # drop this (too many categories)\n",
    "# ---------------------------\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
    "except:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"latin-1\")\n",
    "\n",
    "print(\"Loaded:\", df.shape)\n",
    "\n",
    "# Features (exclude state + target, drop constants)\n",
    "features = [c for c in df.columns if c not in (TARGET, STATE_COL)]\n",
    "features = [c for c in features if df[c].nunique() > 1]\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Pipelines\n",
    "lr_pipe   = Pipeline([(\"pre\", preprocessor), (\"model\", LinearRegression())])\n",
    "ridge_pipe= Pipeline([(\"pre\", preprocessor), (\"model\", Ridge())])\n",
    "lasso_pipe= Pipeline([(\"pre\", preprocessor), (\"model\", Lasso(max_iter=10000))])\n",
    "rf_pipe   = Pipeline([(\"pre\", preprocessor), (\"model\", RandomForestRegressor(random_state=42))])\n",
    "gbr_pipe  = Pipeline([(\"pre\", preprocessor), (\"model\", GradientBoostingRegressor(random_state=42))])\n",
    "\n",
    "# Param grids\n",
    "ridge_params = {\"model__alpha\": [0.01, 0.1, 1, 10, 50, 100]}\n",
    "lasso_params = {\"model__alpha\": [0.01, 0.1, 1, 10, 50, 100]}\n",
    "rf_params    = {\"model__n_estimators\": [50, 100, 150],\n",
    "                \"model__max_depth\": [None, 5, 10],\n",
    "                \"model__min_samples_split\": [2, 4, 6]}\n",
    "gbr_params   = {\"model__n_estimators\": [50, 100],\n",
    "                \"model__learning_rate\": [0.05, 0.1, 0.2],\n",
    "                \"model__max_depth\": [3, 4, 5]}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Fit & evaluate all models\n",
    "def evaluate_model(name, pipe, params=None):\n",
    "    if params:\n",
    "        search = RandomizedSearchCV(pipe, params, n_iter=min(len(list(params.values())[0]), 6),\n",
    "                                    cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1, random_state=42)\n",
    "        search.fit(X_train, y_train)\n",
    "        best = search.best_estimator_\n",
    "        rmse_cv = np.sqrt(-search.best_score_)\n",
    "        print(f\"{name} best params: {search.best_params_}\")\n",
    "    else:\n",
    "        best = pipe.fit(X_train, y_train)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv,\n",
    "                                 scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "        rmse_cv = np.sqrt(-scores.mean())\n",
    "    print(f\"{name} CV RMSE: {rmse_cv:.4f}\")\n",
    "    return best, rmse_cv\n",
    "\n",
    "models = {}\n",
    "models[\"LinearRegression\"], rmse_lr = evaluate_model(\"LinearRegression\", lr_pipe)\n",
    "models[\"Ridge\"], rmse_ridge = evaluate_model(\"Ridge\", ridge_pipe, ridge_params)\n",
    "models[\"Lasso\"], rmse_lasso = evaluate_model(\"Lasso\", lasso_pipe, lasso_params)\n",
    "models[\"RandomForest\"], rmse_rf = evaluate_model(\"RandomForest\", rf_pipe, rf_params)\n",
    "models[\"GradientBoosting\"], rmse_gbr = evaluate_model(\"GradientBoosting\", gbr_pipe, gbr_params)\n",
    "\n",
    "cv_scores = {\n",
    "    \"LinearRegression\": rmse_lr,\n",
    "    \"Ridge\": rmse_ridge,\n",
    "    \"Lasso\": rmse_lasso,\n",
    "    \"RandomForest\": rmse_rf,\n",
    "    \"GradientBoosting\": rmse_gbr,\n",
    "}\n",
    "print(\"\\nCV RMSE summary:\", cv_scores)\n",
    "\n",
    "best_name = min(cv_scores, key=cv_scores.get)\n",
    "best_model = models[best_name]\n",
    "print(\"Selected best model:\", best_name)\n",
    "\n",
    "# Test evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest metrics for {best_name}: RMSE={rmse_test:.4f}, MAE={mae_test:.4f}, R2={r2_test:.4f}\")\n",
    "\n",
    "# Feature importance (for RF/GBR)\n",
    "def get_feature_names(preprocessor):\n",
    "    num_feats = numeric_features\n",
    "    cat_feats = []\n",
    "    if categorical_features:\n",
    "        ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "        cat_feats = ohe.get_feature_names_out(categorical_features).tolist()\n",
    "    return num_feats + cat_feats\n",
    "\n",
    "if best_name in (\"RandomForest\", \"GradientBoosting\"):\n",
    "    model_obj = best_model.named_steps[\"model\"]\n",
    "    feat_names = get_feature_names(best_model.named_steps[\"pre\"])\n",
    "    importances = model_obj.feature_importances_\n",
    "    fi = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "    fi.to_csv(\"feature_importances.csv\", index=False)\n",
    "    print(\"\\nTop features:\\n\", fi.head(10))\n",
    "\n",
    "# Save model and predictions\n",
    "joblib.dump(best_model, \"/Users/sahitipotini/Downloads/obesity/best_nfhs_model.pkl\")\n",
    "print(\"Model saved as best_nfhs_model.pkl\")\n",
    "\n",
    "test_out = X_test.copy()\n",
    "test_out[TARGET] = y_test.values\n",
    "test_out[\"pred_\" + TARGET] = y_pred\n",
    "test_out.to_csv(\"/Users/sahitipotini/Downloads/obesity/predictions_test_set.csv\", index=False)\n",
    "\n",
    "df_full = df.copy()\n",
    "df_full[\"pred_\" + TARGET] = best_model.predict(df_full[features])\n",
    "df_full.to_csv(\"/Users/sahitipotini/Downloads/obesity/all_predictions.csv\", index=False)\n",
    "print(\"Predictions saved: predictions_test_set.csv & all_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27d7437-95c6-47a6-b042-245ae05ad7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial data with shape: (107, 28)\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: women_overweight_obese_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 5.9760\n",
      "  - Ridge CV RMSE: 5.8868\n",
      "  - Lasso CV RMSE: 6.2271\n",
      "  - RandomForest CV RMSE: 6.3210\n",
      "  - GradientBoosting CV RMSE: 6.5376\n",
      "  --> Best model for 'women_overweight_obese_pct': Ridge\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: men_overweight_obese_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 4.8362\n",
      "  - Ridge CV RMSE: 4.8240\n",
      "  - Lasso CV RMSE: 5.2895\n",
      "  - RandomForest CV RMSE: 5.6541\n",
      "  - GradientBoosting CV RMSE: 5.7537\n",
      "  --> Best model for 'men_overweight_obese_pct': Ridge\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: women_underweight_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 4.1385\n",
      "  - Ridge CV RMSE: 4.0242\n",
      "  - Lasso CV RMSE: 4.1475\n",
      "  - RandomForest CV RMSE: 4.0929\n",
      "  - GradientBoosting CV RMSE: 4.0175\n",
      "  --> Best model for 'women_underweight_pct': GradientBoosting\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: men_underweight_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 3.5522\n",
      "  - Ridge CV RMSE: 3.5522\n",
      "  - Lasso CV RMSE: 3.9614\n",
      "  - RandomForest CV RMSE: 3.6898\n",
      "  - GradientBoosting CV RMSE: 3.7363\n",
      "  --> Best model for 'men_underweight_pct': Ridge\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: women_high_blood_sugar_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 3.3350\n",
      "  - Ridge CV RMSE: 3.2449\n",
      "  - Lasso CV RMSE: 3.8058\n",
      "  - RandomForest CV RMSE: 3.1046\n",
      "  - GradientBoosting CV RMSE: 3.1934\n",
      "  --> Best model for 'women_high_blood_sugar_pct': RandomForest\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: men_high_blood_sugar_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 3.6346\n",
      "  - Ridge CV RMSE: 3.5483\n",
      "  - Lasso CV RMSE: 4.0216\n",
      "  - RandomForest CV RMSE: 3.1146\n",
      "  - GradientBoosting CV RMSE: 2.8870\n",
      "  --> Best model for 'men_high_blood_sugar_pct': GradientBoosting\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: women_high_blood_pressure_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 3.4535\n",
      "  - Ridge CV RMSE: 3.3952\n",
      "  - Lasso CV RMSE: 3.6872\n",
      "  - RandomForest CV RMSE: 3.3678\n",
      "  - GradientBoosting CV RMSE: 3.3620\n",
      "  --> Best model for 'women_high_blood_pressure_pct': GradientBoosting\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: men_high_blood_pressure_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 5.2006\n",
      "  - Ridge CV RMSE: 5.1558\n",
      "  - Lasso CV RMSE: 5.6039\n",
      "  - RandomForest CV RMSE: 5.4047\n",
      "  - GradientBoosting CV RMSE: 5.7780\n",
      "  --> Best model for 'men_high_blood_pressure_pct': Ridge\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: women_tobacco_use_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 10.3253\n",
      "  - Ridge CV RMSE: 9.9764\n",
      "  - Lasso CV RMSE: 10.2050\n",
      "  - RandomForest CV RMSE: 10.9959\n",
      "  - GradientBoosting CV RMSE: 10.2280\n",
      "  --> Best model for 'women_tobacco_use_pct': Ridge\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: men_tobacco_use_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 9.5569\n",
      "  - Ridge CV RMSE: 9.0368\n",
      "  - Lasso CV RMSE: 9.2436\n",
      "  - RandomForest CV RMSE: 7.9865\n",
      "  - GradientBoosting CV RMSE: 7.0150\n",
      "  --> Best model for 'men_tobacco_use_pct': GradientBoosting\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: women_alcohol_use_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 4.0945\n",
      "  - Ridge CV RMSE: 3.9671\n",
      "  - Lasso CV RMSE: 4.8053\n",
      "  - RandomForest CV RMSE: 4.0040\n",
      "  - GradientBoosting CV RMSE: 4.0235\n",
      "  --> Best model for 'women_alcohol_use_pct': Ridge\n",
      "\n",
      "==================================================\n",
      "Processing Target Variable: men_alcohol_use_pct\n",
      "==================================================\n",
      "  - LinearRegression CV RMSE: 10.9078\n",
      "  - Ridge CV RMSE: 10.7313\n",
      "  - Lasso CV RMSE: 11.4024\n",
      "  - RandomForest CV RMSE: 9.8435\n",
      "  - GradientBoosting CV RMSE: 10.0414\n",
      "  --> Best model for 'men_alcohol_use_pct': RandomForest\n",
      "\n",
      "==================================================\n",
      "Pipeline complete.\n",
      "All predictions have been saved to '/Users/sahitipotini/Downloads/obesity/all_predictions_multi.csv'\n",
      "\n",
      "--- Sample of the Final Output File ---\n",
      "                       state   area  women_overweight_obese_pct  \\\n",
      "0  Andaman & Nicobar Islands  Urban                       41.72   \n",
      "1  Andaman & Nicobar Islands  Rural                       35.71   \n",
      "2  Andaman & Nicobar Islands  Total                       38.11   \n",
      "3             Andhra Pradesh  Urban                       44.37   \n",
      "4             Andhra Pradesh  Rural                       32.57   \n",
      "\n",
      "   men_overweight_obese_pct  women_underweight_pct  men_underweight_pct  \\\n",
      "0                     37.04                  11.32                 7.77   \n",
      "1                     50.58                   8.19                 1.57   \n",
      "2                     45.30                   9.44                 3.99   \n",
      "3                     37.74                  11.91                14.96   \n",
      "4                     27.99                  16.15                17.19   \n",
      "\n",
      "   women_high_blood_sugar_pct  men_high_blood_sugar_pct  \\\n",
      "0                       19.64                     19.44   \n",
      "1                       16.19                     17.12   \n",
      "2                       17.47                     17.93   \n",
      "3                       23.24                     24.88   \n",
      "4                       17.90                     20.49   \n",
      "\n",
      "   women_high_blood_pressure_pct  men_high_blood_pressure_pct  ...  \\\n",
      "0                          23.43                        28.17  ...   \n",
      "1                          26.40                        31.24  ...   \n",
      "2                          25.29                        30.16  ...   \n",
      "3                          27.54                        32.16  ...   \n",
      "4                          24.28                        27.64  ...   \n",
      "\n",
      "   pred_women_underweight_pct  pred_men_underweight_pct  \\\n",
      "0                   10.902031                  7.978269   \n",
      "1                    8.417882                  8.537815   \n",
      "2                    9.420636                  8.143256   \n",
      "3                   11.877237                 16.307601   \n",
      "4                   23.344987                 19.050326   \n",
      "\n",
      "   pred_women_high_blood_sugar_pct  pred_men_high_blood_sugar_pct  \\\n",
      "0                          14.1868                      16.141148   \n",
      "1                          16.3249                      17.237663   \n",
      "2                          16.6584                      17.774567   \n",
      "3                          21.6049                      24.772280   \n",
      "4                          14.6720                      14.802048   \n",
      "\n",
      "   pred_women_high_blood_pressure_pct  pred_men_high_blood_pressure_pct  \\\n",
      "0                           24.241145                         29.731376   \n",
      "1                           26.486504                         28.453272   \n",
      "2                           25.182452                         28.531762   \n",
      "3                           27.512502                         27.768726   \n",
      "4                           21.591275                         28.032758   \n",
      "\n",
      "   pred_women_tobacco_use_pct  pred_men_tobacco_use_pct  \\\n",
      "0                   19.139972                 33.434739   \n",
      "1                   31.386128                 66.273741   \n",
      "2                   27.037337                 58.656853   \n",
      "3                    2.218106                 15.639058   \n",
      "4                    0.576334                 35.594549   \n",
      "\n",
      "   pred_women_alcohol_use_pct  pred_men_alcohol_use_pct  \n",
      "0                   -0.257063                   26.1614  \n",
      "1                    3.882987                   37.7038  \n",
      "2                    1.880313                   33.5557  \n",
      "3                    0.938676                   19.5135  \n",
      "4                    3.996358                   28.4609  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Final Predictive Modeling Script for NFHS-5 dataset\n",
    "# Automatically trains on multiple health targets using multiple model types\n",
    "# and saves all predictions to a single file.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------- CONFIGURATION ----------\n",
    "INPUT_CSV = \"/Users/sahitipotini/Downloads/obesity/NFHS_5_cleaned_data.csv\"\n",
    "OUTPUT_CSV = \"/Users/sahitipotini/Downloads/obesity/all_predictions_multi.csv\"\n",
    "STATE_COL = \"state\"\n",
    "\n",
    "# List of all target variables to model\n",
    "TARGET_VARIABLES = [\n",
    "    'women_overweight_obese_pct', 'men_overweight_obese_pct',\n",
    "    'women_underweight_pct', 'men_underweight_pct',\n",
    "    'women_high_blood_sugar_pct', 'men_high_blood_sugar_pct',\n",
    "    'women_high_blood_pressure_pct', 'men_high_blood_pressure_pct',\n",
    "    'women_tobacco_use_pct', 'men_tobacco_use_pct',\n",
    "    'women_alcohol_use_pct', 'men_alcohol_use_pct'\n",
    "]\n",
    "# ------------------------------------\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at {INPUT_CSV}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"latin-1\")\n",
    "\n",
    "print(f\"Loaded initial data with shape: {df.shape}\")\n",
    "\n",
    "# Define base features (exclude state col and ALL potential targets)\n",
    "base_features = [c for c in df.columns if c not in TARGET_VARIABLES + [STATE_COL]]\n",
    "base_features = [c for c in base_features if df[c].nunique() > 1] # Drop constant columns\n",
    "\n",
    "# Define preprocessor\n",
    "numeric_features = df[base_features].select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = df[base_features].select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Dictionary to store the best trained model for each target\n",
    "best_models = {}\n",
    "\n",
    "# --- Main loop to iterate through each target variable ---\n",
    "for target in TARGET_VARIABLES:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Processing Target Variable: {target}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    df_loop = df.copy()\n",
    "    df_loop.dropna(subset=[target], inplace=True)\n",
    "    \n",
    "    if df_loop.empty:\n",
    "        print(f\"Skipping '{target}' due to no available data after dropping NaNs.\")\n",
    "        continue\n",
    "\n",
    "    X = df_loop[base_features]\n",
    "    y = df_loop[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # --- Model Selection for the current target ---\n",
    "    # ADDED: Ridge, Lasso, and GradientBoostingRegressor to the comparison\n",
    "    models_to_evaluate = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(random_state=42),\n",
    "        \"Lasso\": Lasso(random_state=42, max_iter=10000),\n",
    "        \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(random_state=42)\n",
    "    }\n",
    "    \n",
    "    cv_scores = {}\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models_to_evaluate.items():\n",
    "        pipe = Pipeline([(\"pre\", preprocessor), (\"model\", model)])\n",
    "        \n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv,\n",
    "                                 scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "        rmse_cv = np.sqrt(-scores.mean())\n",
    "        cv_scores[name] = rmse_cv\n",
    "        print(f\"  - {name} CV RMSE: {rmse_cv:.4f}\")\n",
    "\n",
    "    best_model_name = min(cv_scores, key=cv_scores.get)\n",
    "    print(f\"  --> Best model for '{target}': {best_model_name}\")\n",
    "\n",
    "    best_model_pipeline = Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"model\", models_to_evaluate[best_model_name])\n",
    "    ])\n",
    "    best_model_pipeline.fit(X_train, y_train)\n",
    "    best_models[target] = best_model_pipeline\n",
    "\n",
    "    predictions = best_model_pipeline.predict(df[base_features])\n",
    "    df[f\"pred_{target}\"] = predictions\n",
    "\n",
    "# --- Final Step: Save the consolidated predictions ---\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Pipeline complete.\")\n",
    "print(f\"All predictions have been saved to '{OUTPUT_CSV}'\")\n",
    "\n",
    "# Display the head of the final dataframe with original and new prediction columns\n",
    "final_columns_to_show = ['state', 'area'] + [t for t in TARGET_VARIABLES if f\"pred_{t}\" in df.columns] + [f\"pred_{t}\" for t in TARGET_VARIABLES if f\"pred_{t}\" in df.columns]\n",
    "print(\"\\n--- Sample of the Final Output File ---\")\n",
    "print(df[final_columns_to_show].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599ab01-3e07-47bd-87f9-5147b1c5743b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
